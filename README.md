Emotion Recognition from Speech
ğŸ“‹ Project Overview
This project implements a machine learning model to recognize human emotions from speech audio. The system analyzes audio features to classify emotions like happiness, sadness, anger, and neutral states.
ğŸ¯ Objective
Automatically identify and classify emotions from speech audio using deep learning and signal processing techniques.
ğŸ”§ Technologies Used

Python 3.x
librosa - Audio processing and feature extraction
scikit-learn - Machine learning algorithms
TensorFlow/Keras - Deep learning models
numpy - Numerical computing
pandas - Data manipulation
matplotlib - Data visualization

ğŸµ Audio Features

MFCC (Mel-Frequency Cepstral Coefficients): 13 coefficients
Sampling Rate: 22,050 Hz
Audio Duration: 3 seconds per sample
Feature Extraction: Statistical measures (mean, std, etc.)

ğŸ“Š Emotion Classes

Happy ğŸ˜Š
Sad ğŸ˜¢
Angry ğŸ˜ 
Neutral ğŸ˜

ğŸ¤– Model Architecture

Algorithm: Random Forest Classifier (baseline)
Deep Learning: CNN/RNN/LSTM for advanced implementation
Input Features: 13 MFCC coefficients
Output: 4 emotion classes
Performance Metrics: Accuracy, Precision, Recall, F1-Score
